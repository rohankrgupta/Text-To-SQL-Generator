{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkzTcYZCmdIG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate sentencepiece -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force a clean re-installation of the key libraries\n",
        "!pip uninstall -y transformers accelerate datasets\n",
        "!pip install transformers accelerate datasets"
      ],
      "metadata": {
        "id": "7a_m-uNk6u8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(\"Please log in to your Hugging Face account.\")\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "WAKbIU6Znilh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "kVr0VanRksa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mMyIhz3LoSbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizer\n",
        "\n",
        "# --- 1. Load the dataset from the Hugging Face Hub ---\n",
        "try:\n",
        "    full_dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load dataset. Error: {e}\")\n",
        "\n",
        "# --- 2. Create our small \"smoke test\" sample ---\n",
        "# We'll work with just 200 examples to ensure our pipeline works quickly.\n",
        "smoke_test_sample = full_dataset['train'].select(range(200))\n",
        "print(f\"\\nCreated a smoke test sample with {len(smoke_test_sample)} examples.\")\n",
        "\n",
        "# --- 3. Load the Tokenizer ---\n",
        "# We need the tokenizer that matches our model (BART). It will convert\n",
        "# our text into numerical IDs that the model can understand.\n",
        "model_checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
        "print(f\"\\nTokenizer for '{model_checkpoint}' loaded.\")\n",
        "\n",
        "# --- 4. Define the Preprocessing Function ---\n",
        "# This is the most important step for data preparation.\n",
        "# We format the input as \"Schema: [SCHEMA] | Question: [QUESTION]\"\n",
        "# and the output (labels) as the corresponding SQL query.\n",
        "def preprocess_function(examples):\n",
        "    # The 'sql_context' field holds the 'CREATE TABLE...' schema. 'sql_prompt' is the user question.\n",
        "    inputs = [f\"Schema: {schema} | Question: {question}\" for schema, question in zip(examples['sql_context'], examples['sql_prompt'])]\n",
        "    # The 'sql' field is our target.\n",
        "    targets = [query for query in examples['sql']]\n",
        "\n",
        "    # Tokenize the inputs and targets\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # The model expects the tokenized targets to be in the 'labels' key\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# --- 5. Apply the function to our smoke test sample ---\n",
        "# The .map() function efficiently applies our preprocessing to all examples.\n",
        "tokenized_smoke_test_sample = smoke_test_sample.map(preprocess_function, batched=True)\n",
        "print(\"\\nPreprocessing complete! Our data is now tokenized and ready for training.\")\n",
        "\n",
        "# Let's inspect one processed example to see the result\n",
        "print(\"\\n--- Example of a Processed Data Point ---\")\n",
        "# It now includes 'input_ids', 'attention_mask', and 'labels'\n",
        "print(tokenized_smoke_test_sample[0].keys())"
      ],
      "metadata": {
        "id": "jFvQrGryoTt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88b21de7"
      },
      "source": [
        "from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "# --- 1. Load the Pre-trained Model ---\n",
        "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "print(\"Pre-trained BART model loaded.\")\n",
        "\n",
        "# --- 2. Define Training Arguments (with your username) ---\n",
        "hub_model_id = \"rkgupta3/bart-base-text-to-sql-smoke-test\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bart-text-to-sql-trainer\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=hub_model_id,\n",
        "    hub_strategy=\"every_save\",\n",
        ")\n",
        "print(\"\\nTraining arguments configured.\")\n",
        "\n",
        "# --- 3. Create the Trainer Instance ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_smoke_test_sample,\n",
        ")\n",
        "print(\"Trainer instance created. Starting training...\")\n",
        "\n",
        "# --- 4. Launch Fine-Tuning! ---\n",
        "trainer.train()\n",
        "print(\"\\n--- Training Complete! ---\")\n",
        "\n",
        "# --- 5. Manually save the tokenizer (THE FIX) ---\n",
        "# This crucial step saves vocab.json and other files to the output directory.\n",
        "output_dir = training_args.output_dir\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Tokenizer explicitly saved to {output_dir}\")\n",
        "\n",
        "# --- 6. Push everything (model + tokenizer) to the Hub ---\n",
        "# Now, push_to_hub will find and upload all the necessary files.\n",
        "trainer.push_to_hub(\"Training complete with tokenizer files!\")\n",
        "print(f\"Model and tokenizer successfully pushed to the Hub at: https://huggingface.co/{hub_model_id}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# --- 1. Load your fine-tuned model and tokenizer from the Hub ---\n",
        "# Replace this with the model ID from your successful training run.\n",
        "# This should be the same as the 'hub_model_id' you defined earlier.\n",
        "model_id = \"rkgupta3/bart-base-text-to-sql-smoke-test\"\n",
        "\n",
        "print(f\"Loading model '{model_id}' from the Hub...\")\n",
        "model = BartForConditionalGeneration.from_pretrained(model_id)\n",
        "# Added revision='main'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_id)\n",
        "print(\"Model and tokenizer loaded successfully!\")\n",
        "\n",
        "# --- 2. Define a sample schema and a question ---\n",
        "# Let's test it with a simple schema it has likely seen during training.\n",
        "# You can find more examples in the GretelAI dataset to test with.\n",
        "db_schema = \"\"\"CREATE TABLE artists (\n",
        "  `Artist_ID` real,\n",
        "  `Artist_Name` text,\n",
        "  `Age` real,\n",
        "  `Famous_for` text,\n",
        "  `Birth_Year` real\n",
        ")\"\"\"\n",
        "\n",
        "question = \"What are the names of all artists older than 25?\"\n",
        "print(f\"\\nSchema: {db_schema}\")\n",
        "print(f\"Question: {question}\")\n",
        "\n",
        "# --- 3. Prepare the input for the model ---\n",
        "# We must format the input exactly as we did during training.\n",
        "prompt = f\"Schema: {db_schema} | Question: {question}\"\n",
        "\n",
        "# --- 4. Generate the SQL query ---\n",
        "# Tokenize the prompt and pass it to the model's generate() function.\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=128)\n",
        "\n",
        "# Decode the generated token IDs back into a text string\n",
        "generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- Generated SQL ---\")\n",
        "print(generated_sql)\n",
        "\n",
        "# --- A second, more complex example ---\n",
        "question_2 = \"Return the name and birth year of the youngest artist.\"\n",
        "print(f\"\\nQuestion: {question_2}\")\n",
        "\n",
        "prompt_2 = f\"Schema: {db_schema} | Question: {question_2}\"\n",
        "inputs_2 = tokenizer(prompt_2, return_tensors=\"pt\")\n",
        "outputs_2 = model.generate(**inputs_2, max_length=128)\n",
        "generated_sql_2 = tokenizer.decode(outputs_2[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- Generated SQL ---\")\n",
        "print(generated_sql_2)"
      ],
      "metadata": {
        "id": "h2VUKy0ErtXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import BartTokenizer\n",
        "\n",
        "# --- 1. Load the full dataset from the Hub ---\n",
        "print(\"Loading the full gretelai/synthetic_text_to_sql dataset...\")\n",
        "full_dataset = load_dataset(\"gretelai/synthetic_text_to_sql\", split='train')\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "# --- 2. Create larger, dedicated training and test sets ---\n",
        "# We'll shuffle the data to ensure our splits are random.\n",
        "shuffled_dataset = full_dataset.shuffle(seed=42)\n",
        "\n",
        "# Using 5000 for training and 1000 for testing.\n",
        "train_sample_size = 5000\n",
        "test_sample_size = 1000\n",
        "\n",
        "train_dataset = shuffled_dataset.select(range(train_sample_size))\n",
        "test_dataset = shuffled_dataset.select(range(train_sample_size, train_sample_size + test_sample_size))\n",
        "\n",
        "# It's good practice to bundle them into a single DatasetDict\n",
        "split_datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(f\"\\nCreated a training set with {len(split_datasets['train'])} examples.\")\n",
        "print(f\"Created a test set with {len(split_datasets['test'])} examples.\")\n",
        "\n",
        "# --- 3. Load the Tokenizer ---\n",
        "# We use the same tokenizer as before.\n",
        "model_checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
        "print(f\"\\nTokenizer for '{model_checkpoint}' loaded.\")\n",
        "\n",
        "# --- 4. Define the same Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"Schema: {schema} | Question: {question}\" for schema, question in zip(examples['sql_context'], examples['sql_prompt'])]\n",
        "    targets = [query for query in examples['sql']]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# --- 5. Apply the function to both train and test splits ---\n",
        "print(\"\\nTokenizing the datasets... (This may take a minute)\")\n",
        "tokenized_datasets = split_datasets.map(preprocess_function, batched=True)\n",
        "print(\"Tokenization complete!\")\n",
        "\n",
        "# You can inspect the result to see the structure\n",
        "print(\"\\n--- Processed Datasets Structure ---\")\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "g_axlAdA4aeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "# --- 1. Load the Pre-trained Model ---\n",
        "# We always start from the original pre-trained model, not our smoke-test one.\n",
        "model_checkpoint = \"facebook/bart-base\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "print(\"Pre-trained BART model loaded.\")\n",
        "\n",
        "# --- 2. Define Training Arguments for the Full Run ---\n",
        "# IMPORTANT: Replace 'your-hf-username' with your actual Hugging Face username.\n",
        "hub_model_id = \"rkgupta3/bart-base-text-to-sql-full\" # <- NEW MODEL NAME\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bart-text-to-sql-trainer-full\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs-full',\n",
        "    logging_steps=100,\n",
        "\n",
        "    # Evaluation and Saving Strategy (Corrected Names)\n",
        "    eval_strategy=\"epoch\",     # <- RENAMED from evaluation_strategy\n",
        "    save_strategy=\"epoch\",     # <- Corrected name\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # Hub Integration\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=hub_model_id,\n",
        "    hub_strategy=\"every_save\",\n",
        ")\n",
        "print(\"\\nTraining arguments configured for the full run.\")\n",
        "\n",
        "# --- 3. Create the Trainer Instance with Full Datasets ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'], # <- USE THE FULL 5k TRAINING SET\n",
        "    eval_dataset=tokenized_datasets['test'],   # <- USE THE 1k TEST SET FOR EVALUATION\n",
        "    tokenizer=tokenizer,                       # Pass the tokenizer to ensure it's saved correctly\n",
        ")\n",
        "print(\"Trainer instance created. Starting the full training run... ðŸš€\")\n",
        "\n",
        "# --- 4. Launch Fine-Tuning! ---\n",
        "trainer.train()\n",
        "print(\"\\n--- Full Training Complete! ---\")\n",
        "\n",
        "# --- 5. Push the final best model to the Hub ---\n",
        "# The Trainer automatically pushes the best model because of our settings.\n",
        "# This final push ensures the latest version is uploaded.\n",
        "trainer.push_to_hub(\"Full training of bart-base-text-to-sql complete!\")\n",
        "print(f\"Model successfully pushed to the Hub at: https://huggingface.co/{hub_model_id}\")"
      ],
      "metadata": {
        "id": "jv4_0bHh6FUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = split_datasets['test']\n",
        "for example in test_dataset:\n",
        "    print(example)\n",
        "    break"
      ],
      "metadata": {
        "id": "WQLmu1esWq85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from tqdm import tqdm\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# --- 1. Load Your Fine-Tuned Model and Tokenizer ---\n",
        "# IMPORTANT: Replace this with the model ID from your successful training run.\n",
        "# It should be 'your-hf-username/bart-base-text-to-sql-full'.\n",
        "model_id = \"rkgupta3/bart-base-text-to-sql-full\"\n",
        "\n",
        "print(f\"Loading model '{model_id}' from the Hub...\")\n",
        "try:\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_id)\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_id)\n",
        "    print(\"Model and tokenizer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Stop execution if the model can't be loaded\n",
        "    raise\n",
        "\n",
        "# --- 2. Set up the evaluation ---\n",
        "# We use the non-tokenized test set because we need the raw text\n",
        "test_dataset = split_datasets['test']\n",
        "num_correct = 0\n",
        "num_total = len(test_dataset)\n",
        "\n",
        "print(f\"\\nStarting evaluation on {num_total} test examples...\")\n",
        "\n",
        "# --- 3. Loop through the test set ---\n",
        "for example in tqdm(test_dataset, desc=\"Evaluating Execution Accuracy\"):\n",
        "    # Get the necessary data from the example\n",
        "    context_sql = example['sql_context'] # The CREATE + INSERT statements\n",
        "    question = example['sql_prompt']\n",
        "    ground_truth_sql = example['sql']\n",
        "\n",
        "    # Prepare the prompt for your model\n",
        "    prompt = f\"Schema: {context_sql} | Question: {question}\"\n",
        "\n",
        "    # Generate SQL from your model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=256)\n",
        "    predicted_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- 4. Execute and Compare ---\n",
        "    try:\n",
        "        # Create a temporary in-memory database\n",
        "        conn = sqlite3.connect(':memory:')\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Populate the database with the schema and data\n",
        "        cursor.executescript(context_sql)\n",
        "\n",
        "        # Execute the ground truth query\n",
        "        cursor.execute(ground_truth_sql)\n",
        "        ground_truth_results = cursor.fetchall()\n",
        "\n",
        "        # Execute the predicted query\n",
        "        cursor.execute(predicted_sql)\n",
        "        predicted_results = cursor.fetchall()\n",
        "\n",
        "        # Compare results as unordered sets to handle row order differences\n",
        "        if set(predicted_results) == set(ground_truth_results):\n",
        "            num_correct += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any SQL error occurs, it's considered an incorrect prediction\n",
        "        pass\n",
        "    finally:\n",
        "        # Ensure the connection is always closed\n",
        "        if 'conn' in locals() and conn:\n",
        "            conn.close()\n",
        "\n",
        "# --- 5. Calculate and Print Final Score ---\n",
        "accuracy = (num_correct / num_total) * 100\n",
        "print(\"\\n--- Evaluation Complete! ---\")\n",
        "print(f\"Correct Predictions: {num_correct} / {num_total}\")\n",
        "print(f\"Execution Accuracy: {accuracy:.2f}% ðŸŽ¯\")"
      ],
      "metadata": {
        "id": "eWWIbi32WLvn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}